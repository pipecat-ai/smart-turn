import torch
from torch import nn
import torch.nn.functional as F
from transformers import Wav2Vec2PreTrainedModel, Wav2Vec2Model, AutoConfig, AutoFeatureExtractor
from optimum.exporters.onnx import export
from optimum.exporters.onnx.config import OnnxConfig
from optimum.utils.input_generators import DummyInputGenerator, DummyAudioInputGenerator
from optimum.utils.normalized_config import NormalizedConfig
from collections import OrderedDict
import os
import numpy as np
from pathlib import Path
import argparse
import sys

ROOT = Path(__file__).resolve().parents[1]
DEFAULT_MODEL_DIR = ROOT / "v2-model"
DEFAULT_ONNX_DIR = ROOT / "smart-turn-v2-onnx"

class DummyAttentionMaskGenerator(DummyInputGenerator):
    """
    A dummy input generator that creates an attention mask for audio models.
    """
    SUPPORTED_INPUT_NAMES = ("attention_mask",)

    def __init__(self, task: str, normalized_config: NormalizedConfig, **kwargs):
        """
        This __init__ method accepts the arguments passed by the exporter.
        They are not used in this class but are required by the API.
        """
        self.sequence_length = int(
            normalized_config.sampling_rate * normalized_config.dummy_audio_duration
        )

    def generate(self, batch_size: int, framework: str = "pt", **kwargs):
        """
        Generates a dummy attention_mask. It calculates the sequence_length internally
        to match the length generated by the DummyAudioInputGenerator.
        """
        if isinstance(batch_size, str):
            batch_size = 1
            
        shape = (batch_size, self.sequence_length)
        return torch.ones(*shape, dtype=torch.int64)

class Wav2Vec2ForEndpointing(Wav2Vec2PreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.wav2vec2 = Wav2Vec2Model(config)

        self.pool_attention = nn.Sequential(
            nn.Linear(config.hidden_size, 256),
            nn.Tanh(),
            nn.Linear(256, 1)
        )

        self.classifier = nn.Sequential(
            nn.Linear(config.hidden_size, 256),
            nn.LayerNorm(256),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(256, 64),
            nn.GELU(),
            nn.Linear(64, 1)
        )

        self.init_weights()

    def _init_weights(self, module):
        """Initialize the weights."""
        if isinstance(module, nn.Linear):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.bias is not None:
                module.bias.data.zero_()

    def attention_pool(self, hidden_states, attention_mask):
        attention_weights = self.pool_attention(hidden_states)

        if attention_mask is None:
            raise ValueError("attention_mask must be provided for attention pooling")

        attention_weights = attention_weights + (
                (1.0 - attention_mask.unsqueeze(-1).to(attention_weights.dtype)) * -1e9
        )

        attention_weights = F.softmax(attention_weights, dim=1)
        weighted_sum = torch.sum(hidden_states * attention_weights, dim=1)

        return weighted_sum

    def forward(self, input_values, attention_mask=None, labels=None):
            outputs = self.wav2vec2(input_values, attention_mask=None)
            hidden_states = outputs.last_hidden_state

            if attention_mask is not None:
                input_length = attention_mask.size(1)
                hidden_length = hidden_states.size(1)

                if hidden_length > 0:
                    ratio = input_length / hidden_length
                    indices = (torch.arange(hidden_length, device=attention_mask.device) * ratio).long()
                    pooling_attention_mask = attention_mask[:, indices]
                else:
                    pooling_attention_mask = torch.zeros(hidden_states.shape[:2], device=hidden_states.device)

            else:
                pooling_attention_mask = torch.ones(
                    hidden_states.shape[:2], device=hidden_states.device
                )

            pooled = self.attention_pool(hidden_states, pooling_attention_mask.long())
            logits = self.classifier(pooled)

            if labels is not None:
                pos_weight = ((labels == 0).sum() / (labels == 1).sum()).clamp(min=0.1, max=10.0)
                loss_fct = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
                loss = loss_fct(logits.view(-1), labels.view(-1).float())
                probs = torch.sigmoid(logits.detach())
                return {"loss": loss, "logits": probs}
            
            probs = torch.sigmoid(logits)
            return {"probabilities": probs}

class Wav2Vec2ForEndpointingOnnxConfig(OnnxConfig):
    NORMALIZED_CONFIG_CLASS = NormalizedConfig
    DUMMY_INPUT_GENERATOR_CLASSES = (DummyAudioInputGenerator, DummyAttentionMaskGenerator)


    @property
    def inputs(self) -> "OrderedDict[str, OrderedDict[int, str]]":
        """
        Defines the model's inputs for the ONNX graph.
        """
        return OrderedDict(
            [
                ("input_values", {0: "batch_size", 1: "sequence_length"}),
                ("attention_mask", {0: "batch_size", 1: "sequence_length"}),
            ]
        )

    @property
    def outputs(self) -> "OrderedDict[str, OrderedDict[int, str]]":
        """
        Defines the model's outputs for the ONNX graph.
        """
        return OrderedDict(
            [
                ("probabilities", {0: "batch_size"}),
            ]
        )

def verify_onnx_model(model, onnx_model_path, feature_extractor):
    """
    Verifies that the ONNX model output matches the PyTorch model output.
    """
    print("\n" + "-" * 50)
    print("üöÄ Starting model verification...")
    print("-" * 50)

    try:
        import onnxruntime as ort
    except ImportError:
        print("‚ùå Error: onnxruntime is not installed.")
        print("Please install it with: pip install onnxruntime")
        return

    sampling_rate = feature_extractor.sampling_rate
    dummy_audio = np.random.randn(1, sampling_rate * 2)
    inputs = feature_extractor(
        dummy_audio, sampling_rate=sampling_rate, return_tensors="pt", return_attention_mask=True
    )
    input_values = inputs.input_values
    attention_mask = inputs.attention_mask

    model.eval()
    with torch.no_grad():
        pytorch_outputs = model(input_values=input_values, attention_mask=attention_mask)
        pytorch_probabilities = pytorch_outputs["probabilities"].numpy()

    print(f"PyTorch model output (probabilities): {pytorch_probabilities}")

    ort_session = ort.InferenceSession(str(onnx_model_path))
    onnx_inputs = {
        "input_values": input_values.numpy(),
        "attention_mask": attention_mask.numpy().astype(np.int64)
    }
    onnx_probabilities = ort_session.run(None, onnx_inputs)[0]
    print(f"ONNX model output (probabilities):    {onnx_probabilities}")

    try:
        np.testing.assert_allclose(pytorch_probabilities, onnx_probabilities, rtol=1e-3, atol=1e-4)
        print("\n‚úÖ SUCCESS: The ONNX model's output matches the PyTorch model's output.")
        print("The conversion was successful and the model is ready to use.")
    except AssertionError as e:
        print("\n‚ùå FAILURE: The ONNX model's output does NOT match the PyTorch model's output.")
        print("There might be an issue with the conversion process.")
        print(f"Assertion Error: {e}")
    
    print("-" * 50)


def main(model_path: Path, output_path: Path):
    onnx_model_file = output_path / "model.onnx"
    
    if not model_path.exists():
        print(f"Error: Model directory not found at '{model_path}'")
        return

    print(f"Loading custom model from: {model_path}")

    config = AutoConfig.from_pretrained(str(model_path))
    config.register_for_auto_class("AutoModelForAudioClassification")
    Wav2Vec2ForEndpointing.register_for_auto_class("AutoModelForAudioClassification")

    model = Wav2Vec2ForEndpointing.from_pretrained(str(model_path), config=config)
    feature_extractor = AutoFeatureExtractor.from_pretrained(str(model_path))
    
    model.config.sampling_rate = feature_extractor.sampling_rate # this is so the attribute sampling_rate can be found
    model.config.dummy_audio_duration = 2.0 # 2-second dummy clip


    custom_onnx_config = Wav2Vec2ForEndpointingOnnxConfig(model.config, task="audio-classification")

    print("\nStarting ONNX export...")

    export(
        model=model,
        config=custom_onnx_config,
        output=onnx_model_file,
        opset=14,
    )

    print(f"‚úÖ Export complete!")
    print(f"Your ONNX model has been saved to: {onnx_model_file.resolve()}")

    verify_onnx_model(model, onnx_model_file, feature_extractor)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Export Smart-Turn model to ONNX")
    parser.add_argument("--model", default=os.getenv("SMART_TURN_MODEL", DEFAULT_MODEL_DIR), help="Path to fine-tuned model directory")
    parser.add_argument("--out",   default=os.getenv("SMART_TURN_ONNX", DEFAULT_ONNX_DIR), help="Directory to write ONNX model")
    args = parser.parse_args()

    try:
        import onnxruntime  # noqa: F401 ‚Äì check availability
    except ImportError:
        print("‚ùå  onnxruntime not installed.  pip install onnxruntime")
        sys.exit(1)

    out_dir = Path(args.out)
    out_dir.mkdir(parents=True, exist_ok=True)

    main(Path(args.model), out_dir)
